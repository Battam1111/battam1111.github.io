<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/PersonalAlbums/pic1.jpg"/><link rel="stylesheet" href="/_next/static/css/8205caad36f56fa2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-06092e130ec5d72b.js"/><script src="/_next/static/chunks/4bd1b696-a7d508681645f65f.js" async=""></script><script src="/_next/static/chunks/684-ff5ea1206ba7c347.js" async=""></script><script src="/_next/static/chunks/main-app-b02d57b52e73260c.js" async=""></script><script src="/_next/static/chunks/ca377847-db86cbd8e4cafe79.js" async=""></script><script src="/_next/static/chunks/874-6052627df6fde20c.js" async=""></script><script src="/_next/static/chunks/557-8b78bd991cfa7365.js" async=""></script><script src="/_next/static/chunks/app/page-312a1390a3c2322a.js" async=""></script><meta name="next-size-adjust" content=""/><title>Yanjun Chen Personal Page</title><meta name="description" content="Yanjun Chen Personal Page"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_4d318d __variable_ea5f4b antialiased"><main class="bg-white text-black px-4 md:px-12 lg:px-24 py-12 space-y-32 max-w-5xl mx-auto"><section id="hero" class="relative min-h-[90vh] overflow-hidden flex flex-col-reverse md:flex-row items-center justify-between px-4 sm:px-8 md:px-16 lg:px-24 py-16 space-y-12 md:space-y-0 animate-fade-in bg-gradient-to-br from-white via-gray-50 to-gray-100 dark:from-[#0a0a0a] dark:to-[#1a1a1a]"><div class="absolute inset-0 -z-10" id="tsparticles"><canvas style="width:100%;height:100%"></canvas></div><div class="flex-1 space-y-5 text-center md:text-left z-10"><h1 class="text-4xl sm:text-5xl md:text-6xl font-extrabold tracking-tight leading-tight  text-gray-900 dark:text-white"><span class="block text-gray-900 dark:text-white sm:inline">Hi, I’m<!-- --> </span><span class="text-purple-600 dark:text-purple-400">Yanjun Chen</span>.</h1><p class="text-lg text-gray-700 dark:text-gray-300">PhD in <strong>RLHF</strong> &amp; <strong>Embodied AI</strong>. | INTJ.<br/><em class="text-gray-500 dark:text-gray-400">Builder of thinking agents.</em></p><p class="text-sm italic text-gray-500 dark:text-gray-400">Let’s explore minds that learn.</p><div class="flex flex-wrap gap-4 pt-6 justify-center md:justify-start"><a href="#contact" class="px-5 py-2 rounded-full border border-gray-300 text-sm font-medium text-gray-800 hover:bg-gray-900 hover:text-white transition dark:border-gray-500 dark:text-white dark:hover:bg-white dark:hover:text-black">Contact Me</a><a href="/resume.pdf" target="_blank" class="px-5 py-2 rounded-full border border-blue-500 text-sm font-medium text-blue-600 hover:bg-blue-600 hover:text-white transition dark:border-blue-400 dark:text-blue-300 dark:hover:bg-blue-300 dark:hover:text-black">View CV</a></div></div><div class="flex-1 flex flex-col items-center relative z-10"><div class="relative w-[260px] sm:w-[300px] md:w-[320px] aspect-[3/4] overflow-hidden rounded-[42%/50%] border-4 border-white dark:border-gray-700 shadow-2xl backdrop-blur-md bg-white/10 backdrop-saturate-200 cursor-pointer"><div class="absolute inset-0" style="opacity:0;transform:scale(1.05)"><img alt="Yanjun Chen 1" decoding="async" data-nimg="fill" class="object-cover object-top" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/PersonalAlbums/pic1.jpg"/></div></div><div class="flex space-x-2 mt-4"><span class="w-2.5 h-2.5 rounded-full transition-all duration-300 bg-blue-600"></span><span class="w-2.5 h-2.5 rounded-full transition-all duration-300 bg-gray-300"></span><span class="w-2.5 h-2.5 rounded-full transition-all duration-300 bg-gray-300"></span></div></div></section><section id="about" class="space-y-12 animate-fade-in"><h2 class="text-3xl font-bold border-b pb-2">About Me</h2><p class="text-gray-700 leading-relaxed text-lg max-w-3xl">I&#x27;m a PhD student based in Hong Kong 🇭🇰, originally from China 🇨🇳. As an INTJ thinker and lifelong learner, I explore the frontiers of Artificial Intelligence with a special focus on RLHF (Reinforcement Learning with Human Feedback) and Embodied AI 🤖. My mind is always seeking structure, clarity, and elegant solutions.</p><div class="grid md:grid-cols-3 gap-6"><div class="p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group"><h3 class="text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition">📍<!-- --> <!-- -->Location</h3><p class="text-gray-600 text-sm mt-1">Based in Hong Kong</p></div><div class="p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group"><h3 class="text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition">🎯<!-- --> <!-- -->MBTI</h3><p class="text-gray-600 text-sm mt-1">INTJ – The Architect</p></div><div class="p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group"><h3 class="text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition">🎓<!-- --> <!-- -->Education</h3><p class="text-gray-600 text-sm mt-1">PhD @ HK PolyU</p></div></div><div class="grid md:grid-cols-2 gap-6"><div class="p-5 border rounded-xl bg-white shadow-sm hover:shadow-md transition group"><h3 class="text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition">💻<!-- --> <!-- -->Programming</h3><ul class="list-disc list-inside text-gray-600 text-sm mt-2 space-y-1"><li>Python 🐍</li><li>C/C++ ⚙️</li></ul></div><div class="p-5 border rounded-xl bg-white shadow-sm hover:shadow-md transition group"><h3 class="text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition">🌐<!-- --> <!-- -->Languages</h3><ul class="list-disc list-inside text-gray-600 text-sm mt-2 space-y-1"><li>Chinese 🇨🇳</li><li>English 🇬🇧</li><li>Japanese 🇯🇵</li></ul></div></div><div><h3 class="text-lg font-semibold mb-3">🎨 Interests</h3><div class="flex flex-wrap gap-2"><span class="px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition">Table Tennis 🏓</span><span class="px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition">Video Games 🎮</span><span class="px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition">KTV 🎤</span><span class="px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition">Science &amp; Tech 📖</span></div></div></section><section class="space-y-12" id="research"><h2 class="text-3xl font-semibold border-b pb-2">Research</h2><div class="relative max-w-md"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search absolute left-3 top-2.5 text-gray-400"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg><input type="text" placeholder="Search by keyword or title..." class="pl-10 pr-4 py-2 border rounded-lg w-full text-sm focus:outline-none focus:ring-2 focus:ring-blue-500" value=""/></div><div class="flex flex-wrap gap-2"><button class="text-xs px-3 py-1 rounded-full border bg-black text-white">All</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">RLHF</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Alignment</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Reward Models</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Multimodal</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Chain-of-Thought</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">3D Reasoning</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Reinforcement Learning</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">SAC</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Control</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">LLMs</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Document MT</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Evaluation</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">CoT Distillation</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Model Compression</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Multi-agent RL</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">Graph Networks</button><button class="text-xs px-3 py-1 rounded-full border bg-gray-50 text-gray-700 hover:bg-gray-100">UAVs</button></div><div class="grid md:grid-cols-2 gap-8"><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">The Accuracy Paradox in RLHF: When Better Reward Models Don&#x27;t Yield Better Language Models</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We uncover a paradox where moderately accurate reward models outperform stronger ones in RLHF training. This challenges the assumption that better reward models yield better langua...<button class="flex items-center text-xs text-blue-600 hover:underline mt-1"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down"><path d="m6 9 6 6 6-6"></path></svg>Show More</button></div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-purple-100 text-purple-700">RLHF</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Alignment</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Reward Models</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2410.06554" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank" class="flex items-center gap-1 text-green-600 hover:text-green-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg>Code</a><a href="https://arxiv.org/abs/2410.06554" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">EMNLP 2024</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->1<!-- -->×</span></div></div><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">Integrating Chain-of-Thought for Multimodal Alignment: A Study on 3D Vision-Language Learning</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We explore integrating CoT into 3D vision-language alignment, showing significant gains through structured reasoning with our 3D-CoT benchmark.</div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-pink-100 text-pink-700">Multimodal</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-blue-100 text-blue-700">Chain-of-Thought</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">3D Reasoning</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2503.06232" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://arxiv.org/abs/2503.06232" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">arXiv 2025</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->0<!-- -->×</span></div></div><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">Corrected Soft Actor Critic for Continuous Control</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We improve SAC by correcting action sampling bias introduced by tanh, achieving better convergence and performance on standard benchmarks.</div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Reinforcement Learning</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-green-100 text-green-800">SAC</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Control</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2410.16739" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://arxiv.org/abs/2410.16739" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">arXiv 2024</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->0<!-- -->×</span></div></div><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning—But BLEU Turns a Blind Eye</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We show instruction-tuned LLMs excel in docMT without fine-tuning. BLEU fails to capture improvements, and GPT-4 proves to be a better evaluator.</div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-yellow-100 text-yellow-800">LLMs</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Document MT</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Evaluation</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2410.20941" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://github.com/EIT-NLP/BLEUless_DocMT" target="_blank" class="flex items-center gap-1 text-green-600 hover:text-green-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg>Code</a><a href="https://arxiv.org/abs/2410.20941" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">arXiv 2024</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->2<!-- -->×</span></div></div><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We dissect how granularity, supervision format, and teacher models affect CoT distillation into small language models across 7 datasets.</div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-yellow-100 text-yellow-800">LLMs</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">CoT Distillation</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">Model Compression</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2502.18001" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://github.com/EIT-NLP/Distilling-CoT-Reasoning" target="_blank" class="flex items-center gap-1 text-green-600 hover:text-green-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg>Code</a><a href="https://arxiv.org/abs/2502.18001" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">arXiv 2025</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->4<!-- -->×</span></div></div><div class="border p-6 rounded-xl shadow-sm bg-white space-y-4 hover:shadow-lg hover:-translate-y-1 transition duration-300"><h3 class="text-xl font-bold tracking-tight">Breaking the Pre-Planning Barrier: Real-Time Adaptive Coordination of Mission and Charging UAVs Using Graph RL</h3><div class="text-gray-700 leading-snug text-sm space-y-1">We introduce HGAM, a novel heterogeneous graph-based multi-agent RL model that enables real-time UAV coordination without pre-planned paths.</div><div class="flex flex-wrap gap-2"><span class="text-xs px-2 py-1 rounded-full font-medium bg-orange-100 text-orange-800">Multi-agent RL</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-teal-100 text-teal-800">Graph Networks</span><span class="text-xs px-2 py-1 rounded-full font-medium bg-gray-100 text-gray-700">UAVs</span></div><div class="flex flex-wrap items-center gap-4 pt-2 text-sm"><a href="https://arxiv.org/pdf/2501.14488" target="_blank" class="flex items-center gap-1 text-blue-600 hover:text-blue-800 transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>PDF</a><a href="https://arxiv.org/abs/2501.14488" target="_blank" class="flex items-center gap-1 text-gray-600 hover:text-black transition"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>arXiv</a></div><div class="text-xs text-gray-500 pt-2 flex justify-between items-center"><span class="italic">arXiv 2025</span><span class="hover:text-black transition" title="Last citation update: 2025">Cited <!-- -->0<!-- -->×</span></div></div></div></section><section id="blog" class="space-y-8 animate-fade-in"><div class="flex justify-between items-center"><h2 class="text-3xl font-bold border-b pb-2">📝 Blog</h2><a class="text-sm font-medium text-blue-600 hover:text-blue-800 transition" href="/blog/">View All →</a></div><div class="grid sm:grid-cols-1 md:grid-cols-2 gap-6"><a class="block group" href="/blog/cot-distillation/"><article class="p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1"><h3 class="text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors">Test1</h3><div class="flex justify-between items-center text-sm text-gray-500"><span>2025-02-28</span><div class="flex flex-wrap gap-1"><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">LLMs</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">CoT Distillation</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">Model Compression</span></div></div><div class="h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1"></div><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">How granularity, supervision format, and teacher models affect CoT distillation into smaller LMs across 7 benchmarks.</p><div class="pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition">Read more<svg class="ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M9 5l7 7-7 7"></path></svg></div></article></a><a class="block group" href="/blog/rlhf-philosophy/"><article class="p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1"><h3 class="text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors">Test3</h3><div class="flex justify-between items-center text-sm text-gray-500"><span>2024-03-12</span><div class="flex flex-wrap gap-1"><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">RLHF</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">Philosophy</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">AI Ethics</span></div></div><div class="h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1"></div><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Reinforcement Learning from Human Feedback isn’t just a technical optimization, but a philosophical imperative in agent design.</p><div class="pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition">Read more<svg class="ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M9 5l7 7-7 7"></path></svg></div></article></a><a class="block group" href="/blog/embodied-cognition/"><article class="p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1"><h3 class="text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors">Test2</h3><div class="flex justify-between items-center text-sm text-gray-500"><span>2024-01-25</span><div class="flex flex-wrap gap-1"><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">Embodied AI</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">Neuroscience</span><span class="px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium">RL</span></div></div><div class="h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1"></div><p class="text-gray-700 text-sm leading-relaxed line-clamp-3">Why grounding intelligence in a physical world is essential for the next generation of agents.</p><div class="pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition">Read more<svg class="ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M9 5l7 7-7 7"></path></svg></div></article></a></div></section><section class="space-y-6" id="contact"><h2 class="text-3xl font-semibold border-b pb-2">Contact</h2><p class="text-gray-700 max-w-xl leading-relaxed">I&#x27;m always open to collaboration, interesting conversations, or just sharing ideas over coffee ☕. Whether it&#x27;s about reinforcement learning, embodied AI, or something entirely unexpected — feel free to reach out!</p><div class="space-y-4"><a href="mailto:yan-jun.chen@connect.polyu.hk" class="block hover:underline"><div class="flex items-center justify-between group"><div class="flex items-center space-x-3 text-gray-700 group-hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg><span class="font-medium">Email<!-- -->:</span><span class="text-sm">yan-jun.chen@connect.polyu.hk</span></div><button aria-label="Copy Email" title="Copy" class="ml-4 text-gray-400 hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></a><div class="flex items-center justify-between group"><div class="flex items-center space-x-3 text-gray-700 group-hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-square"><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg><span class="font-medium">WeChat<!-- -->:</span><span class="text-sm">xzqm13143609845</span></div><button aria-label="Copy WeChat" title="Copy" class="ml-4 text-gray-400 hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div><div class="flex space-x-6 pt-6"><a href="https://github.com/Battam1111" target="_blank" rel="noopener noreferrer" aria-label="GitHub" title="GitHub" class="text-gray-600 hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://scholar.google.com.hk/citations?user=Zg8cX0sAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer" aria-label="Google Scholar" title="Google Scholar" class="text-gray-600 hover:text-black transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-book-open"><path d="M12 7v14"></path><path d="M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z"></path></svg></a></div></section></main><footer class="text-sm text-center text-gray-500 py-8 border-t mt-20"><p>© <!-- -->2025<!-- --> Yanjun Chen. All rights reserved.</p></footer><!--$--><!--/$--><!--$--><!--/$--><script src="/_next/static/chunks/webpack-06092e130ec5d72b.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[2045,[\"647\",\"static/chunks/ca377847-db86cbd8e4cafe79.js\",\"874\",\"static/chunks/874-6052627df6fde20c.js\",\"557\",\"static/chunks/557-8b78bd991cfa7365.js\",\"974\",\"static/chunks/app/page-312a1390a3c2322a.js\"],\"default\"]\n5:I[8993,[\"647\",\"static/chunks/ca377847-db86cbd8e4cafe79.js\",\"874\",\"static/chunks/874-6052627df6fde20c.js\",\"557\",\"static/chunks/557-8b78bd991cfa7365.js\",\"974\",\"static/chunks/app/page-312a1390a3c2322a.js\"],\"default\"]\n6:I[6874,[\"647\",\"static/chunks/ca377847-db86cbd8e4cafe79.js\",\"874\",\"static/chunks/874-6052627df6fde20c.js\",\"557\",\"static/chunks/557-8b78bd991cfa7365.js\",\"974\",\"static/chunks/app/page-312a1390a3c2322a.js\"],\"\"]\n7:I[7053,[\"647\",\"static/chunks/ca377847-db86cbd8e4cafe79.js\",\"874\",\"static/chunks/874-6052627df6fde20c.js\",\"557\",\"static/chunks/557-8b78bd991cfa7365.js\",\"974\",\"static/chunks/app/page-312a1390a3c2322a.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/8205caad36f56fa2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"d92eRcHma95qs0slWqfZd\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8205caad36f56fa2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_4d318d __variable_ea5f4b antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"main\",null,{\"className\":\"bg-white text-black px-4 md:px-12 lg:px-24 py-12 space-y-32 max-w-5xl mx-auto\",\"children\":[[\"$\",\"$L4\",null,{}],[\"$\",\"section\",null,{\"id\":\"about\",\"className\":\"space-y-12 animate-fade-in\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold border-b pb-2\",\"children\":\"About Me\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed text-lg max-w-3xl\",\"children\":\"I'm a PhD student based in Hong Kong 🇭🇰, originally from China 🇨🇳. As an INTJ thinker and lifelong learner, I explore the frontiers of Artificial Intelligence with a special focus on RLHF (Reinforcement Learning with Human Feedback) and Embodied AI 🤖. My mind is always seeking structure, clarity, and elegant solutions.\"}],[\"$\",\"div\",null,{\"className\":\"grid md:grid-cols-3 gap-6\",\"children\":[[\"$\",\"div\",\"Location\",{\"className\":\"p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition\",\"children\":[\"📍\",\" \",\"Location\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 text-sm mt-1\",\"children\":\"Based in Hong Kong\"}]]}],[\"$\",\"div\",\"MBTI\",{\"className\":\"p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition\",\"children\":[\"🎯\",\" \",\"MBTI\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 text-sm mt-1\",\"children\":\"INTJ – The Architect\"}]]}],[\"$\",\"div\",\"Education\",{\"className\":\"p-5 border rounded-xl bg-gray-50 shadow-sm hover:shadow-md transition group\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition\",\"children\":[\"🎓\",\" \",\"Education\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 text-sm mt-1\",\"children\":\"PhD @ HK PolyU\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"grid md:grid-cols-2 gap-6\",\"children\":[[\"$\",\"div\",\"Programming\",{\"className\":\"p-5 border rounded-xl bg-white shadow-sm hover:shadow-md transition group\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition\",\"children\":[\"💻\",\" \",\"Programming\"]}],[\"$\",\"ul\",null,{\"className\":\"list-disc list-inside text-gray-600 text-sm mt-2 space-y-1\",\"children\":[[\"$\",\"li\",\"Python 🐍\",{\"children\":\"Python 🐍\"}],[\"$\",\"li\",\"C/C++ ⚙️\",{\"children\":\"C/C++ ⚙️\"}]]}]]}],[\"$\",\"div\",\"Languages\",{\"className\":\"p-5 border rounded-xl bg-white shadow-sm hover:shadow-md transition group\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold flex items-center gap-1 group-hover:text-blue-700 transition\",\"children\":[\"🌐\",\" \",\"Languages\"]}],[\"$\",\"ul\",null,{\"className\":\"list-disc list-inside text-gray-600 text-sm mt-2 space-y-1\",\"children\":[[\"$\",\"li\",\"Chinese 🇨🇳\",{\"children\":\"Chinese 🇨🇳\"}],[\"$\",\"li\",\"English 🇬🇧\",{\"children\":\"English 🇬🇧\"}],[\"$\",\"li\",\"Japanese 🇯🇵\",{\"children\":\"Japanese 🇯🇵\"}]]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg font-semibold mb-3\",\"children\":\"🎨 Interests\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",\"Table Tennis 🏓\",{\"className\":\"px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition\",\"children\":\"Table Tennis 🏓\"}],[\"$\",\"span\",\"Video Games 🎮\",{\"className\":\"px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition\",\"children\":\"Video Games 🎮\"}],[\"$\",\"span\",\"KTV 🎤\",{\"className\":\"px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition\",\"children\":\"KTV 🎤\"}],[\"$\",\"span\",\"Science \u0026 Tech 📖\",{\"className\":\"px-3 py-1 rounded-full text-sm text-gray-700 bg-gray-100 border hover:bg-gray-200 transition\",\"children\":\"Science \u0026 Tech 📖\"}]]}]]}]]}],[\"$\",\"$L5\",null,{}],[\"$\",\"section\",null,{\"id\":\"blog\",\"className\":\"space-y-8 animate-fade-in\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold border-b pb-2\",\"children\":\"📝 Blog\"}],[\"$\",\"$L6\",null,{\"href\":\"/blog\",\"className\":\"text-sm font-medium text-blue-600 hover:text-blue-800 transition\",\"children\":\"View All →\"}]]}],[\"$\",\"div\",null,{\"className\":\"grid sm:grid-cols-1 md:grid-cols-2 gap-6\",\"children\":[false,[[\"$\",\"$L6\",\"cot-distillation\",{\"href\":\"/blog/cot-distillation\",\"className\":\"block group\",\"children\":[\"$\",\"article\",null,{\"className\":\"p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors\",\"children\":\"Test1\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center text-sm text-gray-500\",\"children\":[[\"$\",\"span\",null,{\"children\":\"2025-02-28\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-1\",\"children\":[[[\"$\",\"span\",\"LLMs\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"LLMs\"}],[\"$\",\"span\",\"CoT Distillation\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"CoT Distillation\"}],[\"$\",\"span\",\"Model Compression\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"Model Compression\"}]],false]}]]}],[\"$\",\"div\",null,{\"className\":\"h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm leading-relaxed line-clamp-3\",\"children\":\"How granularity, supervision format, and teacher models affect CoT distillation into smaller LMs across 7 benchmarks.\"}],[\"$\",\"div\",null,{\"className\":\"pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition\",\"children\":[\"Read more\",[\"$\",\"svg\",null,{\"className\":\"ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M9 5l7 7-7 7\"}]}]]}]]}]}],[\"$\",\"$L6\",\"rlhf-philosophy\",{\"href\":\"/blog/rlhf-philosophy\",\"className\":\"block group\",\"children\":[\"$\",\"article\",null,{\"className\":\"p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors\",\"children\":\"Test3\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center text-sm text-gray-500\",\"children\":[[\"$\",\"span\",null,{\"children\":\"2024-03-12\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-1\",\"children\":[[[\"$\",\"span\",\"RLHF\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"RLHF\"}],[\"$\",\"span\",\"Philosophy\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"Philosophy\"}],[\"$\",\"span\",\"AI Ethics\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"AI Ethics\"}]],false]}]]}],[\"$\",\"div\",null,{\"className\":\"h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm leading-relaxed line-clamp-3\",\"children\":\"Reinforcement Learning from Human Feedback isn’t just a technical optimization, but a philosophical imperative in agent design.\"}],[\"$\",\"div\",null,{\"className\":\"pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition\",\"children\":[\"Read more\",[\"$\",\"svg\",null,{\"className\":\"ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M9 5l7 7-7 7\"}]}]]}]]}]}],[\"$\",\"$L6\",\"embodied-cognition\",{\"href\":\"/blog/embodied-cognition\",\"className\":\"block group\",\"children\":[\"$\",\"article\",null,{\"className\":\"p-6 border rounded-2xl bg-white shadow-sm hover:shadow-lg transition-all duration-300 cursor-pointer space-y-4 hover:-translate-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold tracking-tight group-hover:text-blue-700 transition-colors\",\"children\":\"Test2\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center text-sm text-gray-500\",\"children\":[[\"$\",\"span\",null,{\"children\":\"2024-01-25\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-1\",\"children\":[[[\"$\",\"span\",\"Embodied AI\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"Embodied AI\"}],[\"$\",\"span\",\"Neuroscience\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"Neuroscience\"}],[\"$\",\"span\",\"RL\",{\"className\":\"px-2 py-0.5 bg-gray-100 text-gray-600 rounded-full text-xs font-medium\",\"children\":\"RL\"}]],false]}]]}],[\"$\",\"div\",null,{\"className\":\"h-[1px] bg-gradient-to-r from-gray-200/70 to-transparent my-1\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm leading-relaxed line-clamp-3\",\"children\":\"Why grounding intelligence in a physical world is essential for the next generation of agents.\"}],[\"$\",\"div\",null,{\"className\":\"pt-2 flex items-center text-sm text-blue-600 group-hover:text-blue-800 transition\",\"children\":[\"Read more\",[\"$\",\"svg\",null,{\"className\":\"ml-1 w-4 h-4 transition-transform duration-200 group-hover:translate-x-1\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M9 5l7 7-7 7\"}]}]]}]]}]}]]]}]]}],[\"$\",\"$L7\",null,{}]]}],[\"$\",\"footer\",null,{\"className\":\"text-sm text-center text-gray-500 py-8 border-t mt-20\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2025,\" Yanjun Chen. All rights reserved.\"]}]}]],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"nDiSyiYljyBD8pCtdn3TU\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Yanjun Chen Personal Page\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Yanjun Chen Personal Page\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>