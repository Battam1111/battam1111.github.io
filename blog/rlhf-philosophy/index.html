<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/1891ef27502ee43e.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f055e431c58b907b.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-06092e130ec5d72b.js"/><script src="/_next/static/chunks/4bd1b696-4bf832e5ef20f07d.js" async=""></script><script src="/_next/static/chunks/684-fd2d840461fd3899.js" async=""></script><script src="/_next/static/chunks/main-app-b02d57b52e73260c.js" async=""></script><meta name="next-size-adjust" content=""/><title>Yanjun Chen Personal Page</title><meta name="description" content="Yanjun Chen Personal Page"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_4d318d __variable_ea5f4b antialiased"><article class="prose prose-lg max-w-3xl mx-auto py-12 px-4"><h1 class="text-3xl font-bold">Test3</h1><p class="text-sm text-gray-500 mb-6">2024-03-12</p><div><h2 id="-introduction"><a class="anchor" href="#-introduction">üåå Introduction</a></h2>
<p>Reinforcement Learning from Human Feedback (RLHF) is often viewed through a technical lens ‚Äî as a method to fine-tune large language models (LLMs) to align better with user intent. But under the surface lies a deeper, <strong>philosophical motivation</strong>.</p>
<hr>
<h2 id="-the-reward-fallacy"><a class="anchor" href="#-the-reward-fallacy">üîç The Reward Fallacy</a></h2>
<p>The traditional reward signal is inadequate for capturing human values. It‚Äôs brittle, hackable, and often misaligned. The idea that we can <strong>predefine reward functions</strong> for intelligent agents breaks down in the real world.</p>
<blockquote>
<p>‚ÄúA perfectly rewarded system is not necessarily a morally aligned system.‚Äù</p>
</blockquote>
<hr>
<h2 id="-rlhf-as-dialogical-learning"><a class="anchor" href="#-rlhf-as-dialogical-learning">ü§ù RLHF as Dialogical Learning</a></h2>
<p>Human feedback is not just ‚Äúcorrection‚Äù ‚Äî it‚Äôs conversation. RLHF introduces <strong>human intent into the learning loop</strong>, which makes the agent a <em>collaborative learner</em>, not a reward-maximizer.</p>
<hr>
<h2 id="-conclusion"><a class="anchor" href="#-conclusion">üß† Conclusion</a></h2>
<p>RLHF isn‚Äôt a patch. It‚Äôs a philosophical shift:<br>
From <strong>solving</strong> the reward function ‚Üí to <strong>evolving</strong> through feedback.</p>
<blockquote>
<p>Perhaps intelligence is not about maximizing ‚Äî but <strong>negotiating</strong>.</p>
</blockquote></div></article><!--$--><!--/$--><!--$--><!--/$--><script src="/_next/static/chunks/webpack-06092e130ec5d72b.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[9665,[],\"MetadataBoundary\"]\n7:I[9665,[],\"OutletBoundary\"]\na:I[4911,[],\"AsyncMetadataOutlet\"]\nc:I[9665,[],\"ViewportBoundary\"]\ne:I[6614,[],\"\"]\n:HL[\"/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/1891ef27502ee43e.css\",\"style\"]\n:HL[\"/_next/static/css/f055e431c58b907b.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"DOjI06Y8gZno7oYtW4jUD\",\"p\":\"\",\"c\":[\"\",\"blog\",\"rlhf-philosophy\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"rlhf-philosophy\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1891ef27502ee43e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_4d318d __variable_ea5f4b antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"rlhf-philosophy\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[\"$\",\"$L5\",null,{\"children\":\"$L6\"}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f055e431c58b907b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$L8\",\"$L9\",[\"$\",\"$La\",null,{\"promise\":\"$@b\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"JDM81dlLSfJXPQ0s8HEQx\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:\"$Sreact.suspense\"\n10:I[4911,[],\"AsyncMetadata\"]\n6:[\"$\",\"$f\",null,{\"fallback\":null,\"children\":[\"$\",\"$L10\",null,{\"promise\":\"$@11\"}]}]\n12:T61f,\u003ch2 id=\"-introduction\"\u003e\u003ca class=\"anchor\" href=\"#-introduction\"\u003eüåå Introduction\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning from Human Feedback (RLHF) is often viewed through a technical lens ‚Äî as a method to fine-tune large language models (LLMs) to align better with user intent. But under the surface lies a deeper, \u003cstrong\u003ephilosophical motivation\u003c/strong\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-the-reward-fallacy\"\u003e\u003ca class=\"anchor\" href=\"#-the-reward-fallacy\"\u003eüîç The Reward Fallacy\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe traditional reward signal is inadequate for capturing human values. It‚Äôs brittle, hackable, and often misaligned. The idea that we can \u003cstrong\u003epredefine reward functions\u003c/strong\u003e for intelligent agents breaks down in the real world.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúA perfectly rewarded system is not necessarily a morally aligned system.‚Äù\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2 id=\"-rlhf-as-dialogical-learning\"\u003e\u003ca class=\"anchor\" href=\"#-rlhf-as-dialogical-learning\"\u003eü§ù RLHF as Dialogical Learning\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eHuman feedback is not just ‚Äúcorrection‚Äù ‚Äî it‚Äôs conversation. RLHF introduces \u003cstrong\u003ehuman intent into the learning loop\u003c/strong\u003e, which makes the agent a \u003cem\u003ecollaborative learner\u003c/em\u003e, not a reward-maximizer.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-conclusion\"\u003e\u003ca class=\"anchor\" href=\"#-conclusion\"\u003eüß† Conclusion\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eRLHF isn‚Äôt a patch. It‚Äôs a philosophical shift:\u003cbr\u003e\nFrom \u003cstrong\u003esolving\u003c/strong\u003e the reward function ‚Üí to \u003cstrong\u003eevolving\u003c/strong\u003e through feedback.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePerhaps intelligence is not about maximizing ‚Äî but \u003cstrong\u003enegotiating\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e4:[\"$\",\"article\",null,{\"className\":\"prose prose-lg max-w-3xl mx-auto py-12 px-4\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold\",\"children\":\"Test3\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-500 mb-6\",\"children\":\"2024-03-12\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$12\"}}]]}]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Yanjun Chen Personal Page\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Yanjun Chen Personal Page\"}]],\"error\":null,\"digest\":\"$undefined\"}\nb:{\"metadata\":\"$11:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>