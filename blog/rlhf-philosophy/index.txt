1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
5:I[9665,[],"MetadataBoundary"]
7:I[9665,[],"OutletBoundary"]
a:I[4911,[],"AsyncMetadataOutlet"]
c:I[9665,[],"ViewportBoundary"]
e:I[6614,[],""]
:HL["/_next/static/media/569ce4b8f30dc480-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/8305025519c60134.css","style"]
:HL["/_next/static/css/f055e431c58b907b.css","style"]
0:{"P":null,"b":"lBVhdF-rttuCiBWV3c_R0","p":"","c":["","blog","rlhf-philosophy",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","rlhf-philosophy","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/8305025519c60134.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_4d318d __variable_ea5f4b antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","rlhf-philosophy","d"],["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L4",["$","$L5",null,{"children":"$L6"}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f055e431c58b907b.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L7",null,{"children":["$L8","$L9",["$","$La",null,{"promise":"$@b"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","21iMRY7BJJlsdqk4ErWT4",{"children":[["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$e","$undefined"],"s":false,"S":true}
f:"$Sreact.suspense"
10:I[4911,[],"AsyncMetadata"]
6:["$","$f",null,{"fallback":null,"children":["$","$L10",null,{"promise":"$@11"}]}]
12:T61f,<h2 id="-introduction"><a class="anchor" href="#-introduction">üåå Introduction</a></h2>
<p>Reinforcement Learning from Human Feedback (RLHF) is often viewed through a technical lens ‚Äî as a method to fine-tune large language models (LLMs) to align better with user intent. But under the surface lies a deeper, <strong>philosophical motivation</strong>.</p>
<hr>
<h2 id="-the-reward-fallacy"><a class="anchor" href="#-the-reward-fallacy">üîç The Reward Fallacy</a></h2>
<p>The traditional reward signal is inadequate for capturing human values. It‚Äôs brittle, hackable, and often misaligned. The idea that we can <strong>predefine reward functions</strong> for intelligent agents breaks down in the real world.</p>
<blockquote>
<p>‚ÄúA perfectly rewarded system is not necessarily a morally aligned system.‚Äù</p>
</blockquote>
<hr>
<h2 id="-rlhf-as-dialogical-learning"><a class="anchor" href="#-rlhf-as-dialogical-learning">ü§ù RLHF as Dialogical Learning</a></h2>
<p>Human feedback is not just ‚Äúcorrection‚Äù ‚Äî it‚Äôs conversation. RLHF introduces <strong>human intent into the learning loop</strong>, which makes the agent a <em>collaborative learner</em>, not a reward-maximizer.</p>
<hr>
<h2 id="-conclusion"><a class="anchor" href="#-conclusion">üß† Conclusion</a></h2>
<p>RLHF isn‚Äôt a patch. It‚Äôs a philosophical shift:<br>
From <strong>solving</strong> the reward function ‚Üí to <strong>evolving</strong> through feedback.</p>
<blockquote>
<p>Perhaps intelligence is not about maximizing ‚Äî but <strong>negotiating</strong>.</p>
</blockquote>4:["$","main",null,{"className":"flex flex-col md:flex-row max-w-6xl mx-auto px-4 py-12 space-y-12 md:space-y-0 md:space-x-8 animate-fade-in","children":[["$","aside",null,{"className":"hidden md:block w-64 sticky top-24 self-start text-sm text-gray-500","children":[["$","p",null,{"className":"mb-3 font-semibold text-gray-700","children":"üìö Table of Contents"}],["$","ul",null,{"className":"space-y-2","children":[["$","li",null,{"children":["$","a",null,{"href":"#background","className":"hover:underline","children":"üß© Background"}]}],["$","li",null,{"children":["$","a",null,{"href":"#experiment-design","className":"hover:underline","children":"üß™ Experiment"}]}],["$","li",null,{"children":["$","a",null,{"href":"#key-findings","className":"hover:underline","children":"üìà Findings"}]}],["$","li",null,{"children":["$","a",null,{"href":"#takeaway","className":"hover:underline","children":"üí° Takeaway"}]}]]}]]}],["$","article",null,{"className":"prose prose-lg max-w-none dark:prose-invert","children":[["$","h1",null,{"className":"text-3xl font-bold mb-1","children":"Test3"}],["$","p",null,{"className":"text-sm text-gray-500 mb-6","children":"2024-03-12"}],["$","div",null,{"dangerouslySetInnerHTML":{"__html":"$12"}}],["$","div",null,{"className":"mt-10 border-t pt-6","children":[["$","h2",null,{"className":"text-lg font-semibold mb-2","children":"üëÄ You might also like:"}],["$","ul",null,{"className":"list-disc list-inside text-sm text-blue-600 space-y-1","children":[["$","li",null,{"children":["$","a",null,{"href":"/blog/rlhf-philosophy","className":"hover:underline","children":"RLHF isn‚Äôt optimization, it‚Äôs dialogue"}]}],["$","li",null,{"children":["$","a",null,{"href":"/blog/embodied-cognition","className":"hover:underline","children":"Why AI needs bodies to think"}]}]]}]]}]]}]]}]
9:null
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
8:null
11:{"metadata":[["$","title","0",{"children":"Yanjun Chen Personal Page"}],["$","meta","1",{"name":"description","content":"Yanjun Chen Personal Page"}]],"error":null,"digest":"$undefined"}
b:{"metadata":"$11:metadata","error":null,"digest":"$undefined"}
